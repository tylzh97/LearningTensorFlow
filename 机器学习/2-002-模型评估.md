# 机器学习(Machine Learning)
## 第二章 模型评估

### 评估方法
* 基本概念
  * 测试集(Testing Set) : 测试模型对新样本的泛化能力
    * 测试集中的样本要与训练集中的样本尽量互斥
  * 测试误差(Testing Error) : 泛化误差的近似
* 划分数据集的方法
  * 留出法
    * 直接将数据集划分为两个互斥的集合
    * 在划分时,训练集与测试集划分要尽可能保持数据分布的一致性,从而减少因样本划分不同而引入的误差.
    * 一般而言,需要进行多次随机划分,多次试验取平均值.
    * 训练样本与测试样本比例 2:1~4:1
  * 交叉验证法(Cross Validation)
    * 
    * k折交叉验证:
    <br> 将数据集分层采样,划分成k个大小相同或相似的互斥的子集.每次使用k-1个子集的并集作为训练集;剩余的子集作为测试集.最终返回k个测试结果的均值.(常取k=10)
    <br> 如下图所示,每次迭代的方块数为k;白方块为训练集,蓝方块为测试集. 
    <br> <img src="http://ww1.sinaimg.cn/large/007s1hc9ly1g11hxkip9tj30gu09hdgc.jpg" width="90%" height="90%">
    * 留一验证:
    <br> 即当k=n(样本数)时的k折交叉验证
    <br> 优点:结果准确;不受样本随机划分方式的影响.
    <br> 缺点:当数据集较大时,计算开销难以忍受.
  * 自助法
    * 以自助采样法为基础,对数据集`D`进行有放回采样m次得到训练集`D'`;使用集合`D-D'`作为测试集.
    <br> 优点:从初始数据产生多个不同的训练集,对集成学习有很大的好处.

### 评估指标
* 常用评估指标
  1. 类平衡数据集
  <br> 针对每一类同等重要的情况
     * 准确率
     * 错误率
  2. 类不平衡数据集
     * 正确分类稀有类
     * 查准率/查全率
       <br> 查准率(P):被分为正类的样本中实际为正类样本的比例
       <br> 查全率(R):实际为正类的样本中实际被分为正类的比例
     * 由于查准率与查全率是一对矛盾的查全指标,需要有综合度量方法进行评估
       <br> F1度量(调和均值):F1=2*P*R/(P+R)
       <br> Fβ度量:Fβ=(1+β^2)*P*R/(β^2*P)+R
  3. ROC曲线(受试者工作特征曲线)
      * 根据分类器的概率预测结果对样例排序,并按照此顺序依次选择不同的截断点,逐个吧样例作为正例进行预测.每次计算出当前分类器的真正率(TPR)与假正率(FPR),并以其作为纵轴和横轴绘图
      * AUC,即ROC曲线下与坐标轴围成的面积
      * ROC与AUC用来度量概率分类器的排序性能
  4. CLL(条件似然性)
      * 用于直接度量分类器的类概率估计性能
      * <img src="http://ww1.sinaimg.cn/large/007s1hc9gy1g13r8qpv5kj30mm09bgtp.jpg" width="90%">
      * 

### 比较检验
1. 成对双边t检验
   * 用于在一个数据集上比较两个分类器的性能
   * 对不同的分类器进行k折交叉验证,得到k对测试错误率.对每一对结果求差.通过下图所示表达式即可计算出误差的均值、方差以及t统计量.<br><img src="http://ww1.sinaimg.cn/large/007s1hc9gy1g13re4iw14j30pa0avwqc.jpg"/>
   * 其中,t统计量服从自由度为(k-1)的t分布.有以下关系
     * 如果t值<临界值,两个分类器的性能没有显著差别
     * 如果t值>临界值,两个分类器的性能有显著差别
     * 其中,平均错误率较小的分类器性能较优
     * 临界值可以通过查t分布临界值表得到
2. Friedman检验与Nemenyi后续检验
   * Friedman检验:在一组数据集上比较多个分类器的性能
      1. 使用留出法或交叉验证法,得到每个算法在每个数据集上的测试结果
   * 
##### 抱歉这里没听懂....先听后面的.....